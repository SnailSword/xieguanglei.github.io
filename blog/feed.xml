<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title><![CDATA[一叶斋]]></title>
        <description><![CDATA[一叶障目 一叶知秋]]></description>
        <link>http://xieguanglei.github.io/</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Wed, 05 Dec 2018 15:07:03 GMT</lastBuildDate>
        <atom:link href="http://xieguanglei.github.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[WebGL 纹理详解]]></title>
            <description><![CDATA[<h1 id="webgl-">WebGL 纹理详解</h1>
<p>Buffer（数据缓冲区）与 Texture（纹理）是 WebGL 程序的两大数据来源。Buffer 可以通过 ArrayBuffer 或更语义化的 TypedArray 来构造；而 Texture 在大多数情况下，是通过 Image 对象来构造的。在构造和使用 Texture 的过程中，需要确定很多<strong>选项</strong>来以不同的方式构造 Texture；这些选项之间有着各种各样的关系，或互相依赖，或互相排斥，或互相影响。最近，我又重新梳理了一遍我所用到的 WebGL 纹理各种参数的影响，稍作整理，以防遗忘。</p>
<p>为此，我专门编写了一个 Demo，如下所示。Demo 页的右上角有一个使用 <a href="https://github.com/dataarts/dat.gui">dat.GUI</a> 生成的控件，其中列举了影响纹理的一些选项。这篇文章将逐个讨论这些选项的作用和相互关系。</p>
<p><a class="jsbin-embed" href="https://jsbin.com/boxazam/latest/embed?output&height=512px">JS Bin on jsbin.com</a></p>
<h2 id="wrap">Wrap</h2>
<p>在 JavaScript 中，创建纹理的基本流程大约如下所示：</p>
<pre><code class="lang-javascript">// 激活纹理单元
gl.activeTexture(...);

// 创建和绑定
const texture = gl.createTexture();
gl.bindTexture(gl.TEXTURE_2D, texture);

// 参数设置
gl.texParameteri(...);
gl.texParameteri(...);

// 填充纹理的内容
gl.texImage2D(..., image);

// 通过纹理单元将纹理传送给着色器程序
gl.uniform1i(...);
</code></pre>
<p>然后，在着色器中，使用一个坐标 (x,y) 从纹理上取色。</p>
<pre><code class="lang-glsl">vec4 color = texture2D(texture, vec2(x, y));
</code></pre>
<p>通常，从纹理上取色的坐标 x 和 y 的值均在 0~1 之间。Wrap 配置项规定了当取色坐标的坐标取值在 (0, 1) 之外（即试图从纹理图片之外取色）时，应该如何取色。Wrap 有两种：切割(CLAMP)和重复(REPEAT)。我们可以操作上面的 Demo，调整 scale 的值使得纹理图片缩小一些，然后切换 Wrap 配置项为 CLAMP 或 REPEAT，可以发现：当选项置为 CLAMP 时，从纹理外部取色会落到对应的纹理的边缘，比如 <code>texture2D(texture, vec2(2.2, 0.5))</code> 的值会等于 <code>texture2D(texture, vec2(1.0, 0.5))</code>；而选项置为 REPEAT 时，纹理会「平铺」开来，从外部取色会把取色坐标按1取模，映射到纹理内部，比如 <code>texture2D(texture, vec2(2.2, 0.5))</code> 的值会等于 <code>texture2D(texture, vec2(0.2, 0.5))</code>。</p>
<p>切换 Wrap 配置项的代码如下：</p>
<pre><code class="lang-javascript">// CLAMP
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);

// REPEAT
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.REPEAT);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.REPEAT);
</code></pre>
<p>下图是 REPEAT 的情况：</p>
<p><img src="https://gw.alicdn.com/tfs/TB1XAx8t9zqK1RjSZPcXXbTepXa-600-380.jpg" alt="repeat"></p>
<p>下图是 CLAMP 的情况：</p>
<p><img src="https://gw.alicdn.com/tfs/TB1dgqbt7voK1RjSZFwXXciCFXa-600-375.jpg" alt="clamp"></p>
<blockquote>
<p>有一点需要注意的是，REPEAT 模式对纹理图片的尺寸有要求，宽度和高度必须为 2 的整数次幂，如 32x32 或 1024x256 的图片都是符合要求的，但 500x512 或 300x300 是不符合的。我们可以将控件中的 size 从 512 改成 300，这时 Demo 将加载这张图片的一个尺寸为 300x300 的替代品作为纹理。如果 Wrap 为 CLAMP，我们会发现稍微纹理模糊了一些，但是如果 Wrap 为 REPEAT，则会报警并黑屏。</p>
</blockquote>
<h2 id="flipy">FlipY</h2>
<p>FlipY 是 WebGL 的一个全局配置项（准确地说，它的名称是 UNPACK_FLIP_Y_WEBGL），在本文的范畴中，它主要影响了 <code>texImage2D()</code> 方法的行为。</p>
<p>按照 OpenGL 的惯例，当着色器调用 <code>texture2D(t, st)</code> 方法从纹理中取色时，传入的取色坐标 <code>st</code> 的原点是左下角。换言之，如果传入的坐标是 (0, 0)，那么 OpenGL 期望取到的是左下角的那个像素的颜色。但是，由于在 Web 上图片数据的存储是从左上角开始的，传入坐标 (0,0) 时，实际上会取到左上角坐标的值。如果我们设置了 <code>FlipY</code> 配置项，那么在向纹理中加载数据的时候，就会对数据作一次翻转，使纹理坐标原点变为左下角。</p>
<p>开启 FlipY 的代码是：</p>
<pre><code class="lang-javascript">gl.pixelStorei(gl.UNPACK_FLIP_Y_WEBGL, true);
</code></pre>
<p>在 Demo 中，默认情况下 FlipY 配置项是勾选的，如果你勾选取消，会发现纹理图片被翻转了。下图是取消 FlipY 配置后的情况。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1dFuJtVzqK1RjSZSgXXcpAVXa-600-379.jpg" alt="cancel-flipY"></p>
<h2 id="min_filter-mag_filter">MIN_FILTER 和 MAG_FILTER</h2>
<p>一个纹理是由离散的数据组成的，比如一个 2x2 的纹理是由 4 个像素组成的，使用 (0,0)、(0, 1) 等四个坐标去纹理上取样，自然可以取到对应的像素颜色；但是，如果使用非整数坐标到这个纹理上去取色。比如，当这个纹理被「拉近」之后，在屏幕上占据了 4x4 一共 16 个像素，那么就会使用 (0.33,0) 之类的坐标去取值，如何根据离散的 4 个像素颜色去计算 (0.33,0) 处的颜色，就取决于参数 MAG_FILTER。在 WebGL 中设置这两个项的代码如下所示：</p>
<pre><code class="lang-javascript">gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);
gl.texParameteri(
    gl.TEXTURE_2D, 
    gl.TEXTURE_MIN_FILTER, 
    gl.LINEAR_MIPMAP_LINEAR
);
</code></pre>
<p>MAG_FILTER 有两个可选项，NEAREST 和 LINEAR。顾名思义，NEAREST 就是去取距离当前坐标最近的那个像素的颜色，而 LINEAR 则会根据距离当前坐标最近的 4 个点去内插计算出一个数值，如图所示。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1hgfJt3HqK1RjSZFEXXcGMXXa-600-481.png" alt="MAG_FILTER"></p>
<p>显然 NEAREST 的运行速度更快，但 LINEAR 的效果更好。使用 NEAREST 时，当纹理被拉得比较近，颗粒感会比较明显，而使用 LINEAR 则会顺滑一些。你可以切换右上角空间上的 MAG_FILTER 选项，然后拖动 scale 滑块将纹理放大，体会一下两者的差别。</p>
<p>下图是 MAG_FILTER 为 NEAREST 的效果：</p>
<p><img src="https://gw.alicdn.com/tfs/TB1l_nZtYvpK1RjSZFqXXcXUVXa-600-421.png" alt="MAG_FILTER_NEAREST"></p>
<p>下图是 MAG_FILTER 为 LINEAR 的效果：</p>
<p><img src="https://gw.alicdn.com/tfs/TB1I2_St4TpK1RjSZR0XXbEwXXa-600-419.png" alt="MAG_FILTER_LINEAR"></p>
<p>MAG_FILTER 作用于将纹理拉近/放大的情形，而当纹理远离/缩小的时候，起作用的是 MIN_FILTER。MIN_FILTER 有以下 6 个可选配置项：</p>
<ul>
<li>NEAREST</li>
<li>LINEAR</li>
<li>NEAREST_MIPMAP_NEAREST</li>
<li>NEAREST_MIPMAP_LINEAR</li>
<li>LINEAR_MIPMAP_NEAREST</li>
<li>LINEAR_MIPMAP_LINEAR</li>
</ul>
<p>前两个配置项和 MAG_FILTER 的含义和作用是完全一样的。但问题是，当纹理被缩小时，原纹理中并不是每一个像素周围都会落上采样点，这就导致了某些像素，完全没有参与纹理的计算，新纹理丢失了一些信息。假设一种极端的情况，就是一个纹理彻底缩小为了一个点，那么这个点的值应当是纹理上所有像素颜色的平均值，这才比较合理。但是 NEAREST 只会从纹理中取一个点，而 LINEAR 也只是从纹理中取了四个点计算了一下而已。这时候，就该用上 MIPMAP 了。</p>
<h2 id="mipmap-min_filter">Mipmap 和 MIN_FILTER</h2>
<p>为了在纹理缩小也获得比较好的效果，需要按照采样密度，选择一定数量（通常大于 LINEAR 的 4 个，极端情况下为原纹理上所有像素）的像素进行计算。实时进行计算的开销是很大的，所有有一种称为 MIPMAP（金字塔）的技术。在纹理创建之初，就为纹理创建好 MIPMAP，比如对 512x512 的纹理，依次建立 256x256（称为 1 级 Mipmap）、128x128（称为 2 级 Mipmap） 乃至 2x2、1x1 的纹理。实时渲染时，根据采样密度选择其中的某一级纹理，以此避免运行时的大量计算。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1_0.ot9zqK1RjSZPxXXc4tVXa-501-503.png" alt="Mipmap"></p>
<blockquote>
<p>显而易见的是，使用 MIPMAP 同样需要纹理尺寸为 2 的整数次幂。</p>
</blockquote>
<p>WebGL 中，可以通过调用以下方法来生成 mipmap。</p>
<pre><code class="lang-javascript">gl.generateMipmap(gl.TEXTURE_2D);
</code></pre>
<p>我们将控制面板中的 <code>generateMipmap</code> 打开，并把 scale 调到一个较小的值（比如 0.5），然后切换 MIN_FILTER 的各种设置，就可以观察到 mipmap 的效果了。</p>
<p>下图是 MIN_FILTER 为 LINEAR 的效果（NEAREST 效果是类似的）：</p>
<p><img src="https://gw.alicdn.com/tfs/TB1CmY0tVzqK1RjSZFvXXcB7VXa-600-420.png" alt="MIN_FILTER_LINEAR"></p>
<p>下图是 MIN_FILTER 为 LINEAR_MIPMAP_NEAREST 的效果（LINEAR_MIPMAP_LINEAR 效果是类似的）：</p>
<p><img src="https://gw.alicdn.com/tfs/TB11zLZt9zqK1RjSZPcXXbTepXa-600-418.png" alt="MIN_FILTER_MIPMAP"></p>
<p>可以看到当采用 MIPMAP 时，纹理平滑了很多（特别是头发部分）。</p>
<blockquote>
<p>MIN_FILTER 的默认值是 LINEAR_MIPMAP_NEAREST。在 XXX_MIPMAP_XXX 的格式中，前一个 XXX 表示在单个 MIPMAP 中取色的方式，与单独的 LINEAR 或 NEAREST 类似，而后一个 XXX 表示，随着采样密度连续变化（通常是因为缩放因子连续变化）时，是否在多层 MIPMAP 之间内插。使用 MIPMAP 时，后一个 LINEAR 比较重要，只要后者是 LINEAR，前者的意义其实并不特别大，所以默认选项 NEAREST_MIPMAP_LINEAR 也是最常用的。</p>
</blockquote>
<h2 id="-mipmap">自定义 Mipmap</h2>
<p>我们可以使用 <code>gl.generateMipmap()</code> 方法来根据原始纹理生成一套 mipmap，这种生成的方式是默认的。但是实际上，我们还有一种更灵活的方式来自定义 Mipmap，那就是直接传入另一张图片。比如，这里我们使用的是一张 512x512 的 lena 图，调用 <code>gl.generateMipmap()</code> 会生成 256x256、128x128 直至 1x1 等一系列图片。但是，如果我们手头正好有一张 128x128 尺寸的图片，我们就可以强制指定这张图片作为原始纹理的 1 级 Mipmap。</p>
<p><img src="https://gw.alicdn.com/tfs/TB17fknt9rqK1RjSZK9XXXyypXa-600-503.png" alt="Custom Mipmap"></p>
<p>自定义 Mipmap 纹理的代码如下所示：</p>
<pre><code class="lang-javascript">// 首先要调用此方法
// 如果在 texImage2D 后调用，则定制的 mipmap 会被覆盖
gl.generateTexture(gl.TEXTURE_2D);

gl.texImage2D(
    gl.TEXTURE_2D,
    1,  // 就是这个参数指定几级 Mipmap
    gl.RGBA,
    gl.RGBA,
    gl.UNSIGNED_BYTE,
    image // 尺寸必须严格符合指定级别 Mipmap 应有的尺寸
);
</code></pre>
<p>当我们指定好自定义的 Mipmap 纹理后（即勾选 customMipmap），同时勾选 generateMipmap，并且保证 MIN_FILTER 在 XXX_MIPMAP_XXX 的配置上，此时拖动 scale 滑块，会发现在缩小过程中的某个过程，纹理会变成自定义图片。当 MIN_FILTER 在 XXX_MIPMAP_NEAREST 时，纹理是「突变」的；而当 MIN_FILTER 位于 XXX_MINMAP_LINEAR 时，纹理是「渐变」的，这也印证了前面关于 XXX_MIPMAP_XXX 的解释。</p>
<p>下图是 MIN_FILTER 为 LINEAR_MIPMAP_NEAREST 时缩放的效果：</p>
<p><img src="https://gw.alicdn.com/tfs/TB11r3xt7voK1RjSZFDXXXY3pXa-500-316.gif" alt="CustomMipmap1"></p>
<p>下图是 MIN_FILTER 为 LINEAR_MIPMAP_LINEAR 时缩放的效果：</p>
<p><img src="https://gw.alicdn.com/tfs/TB1ctgvtYvpK1RjSZFqXXcXUVXa-500-316.gif" alt="CustomMipmap2"></p>
<h2 id="srgb-">SRGB 扩展</h2>
<p>RGB 颜色空间是为人眼设计的。但是人眼感光强度和真实光线的物理强度并不是线性关系，这是人眼的生理结构所决定的。比如，在人眼看来，屏幕上显示的白色 #FFFFFF 的亮度，是半灰色 #888888 亮度的 2 倍，但是两者真实的光线物理强度（直射下单位面积上受光照的功率）并不是 2 倍关系。所以，当着色器需要基于一些物理规律来计算颜色时，直接取纹理的颜色没有意义，需要作一次转换。</p>
<p>一个经验是，将人眼感知的颜色归一化到 (0,1) 之间，然后取 2.2 次方幂，得到的结果与光线物理强度是呈线性关系的。如下所示，横轴是 RGB 色彩空间，蓝色直线是人眼的感光曲线，红色曲线是光线物理强度曲线。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1QD7Xt3HqK1RjSZFEXXcGMXXa-297-286.png" alt="SRGB"></p>
<p>SRGB 扩展所做的就是这件事，在用户调用 <code>texImage2D</code> 向纹理中传输数据时，将纹理的每个像素颜色取了自己的 2.2 次方幂。开启 SRGB 扩展的代码如下所示：</p>
<pre><code class="lang-javascript">const SRGBExtension = gl.getExtension(&quot;EXT_SRGB&quot;);

gl.texImage2D(
    gl.TEXTURE_2D, 
    0, 
    // 接下来两个参数原本是 gl.RGBA
    SRGBExtension.SRGB_ALPHA_EXT, 
    SRGBExtension.SRGB_ALPHA_EXT, 
    gl.UNSIGNED_BYTE, 
    image
);
</code></pre>
<p>在控件中勾选 SRGB，会发现纹理变暗了很多，因为颜色作为 [0,1] 之间的数值，取了自己的 2.2 次方幂，自然变小了。其实，如果不使用 SRGB 扩展，在着色器中也可以模拟。我们可以取消勾选 SRGB 选项，然后将 postProcess（后处理）项置为 c^2.2，画面也同样变暗了。postProcess 是这个 Demo 埋在 Shader 中的一个普通 uniform 变量，为最后颜色输出增加一个可选的环节（取自己的 2.2 次方幂，或者取自己的 1/2.2 次方幂）。</p>
<p>着色器中有关 postProcess 的代码如下：</p>
<pre><code class="lang-glsl">if(uPostProcess == 1){
    gl_FragColor = vec4(pow(gl_FragColor.rgb, vec3(1.0/2.2)), 1.0);
}else if(uPostProcess == 2){
    gl_FragColor = vec4(pow(gl_FragColor.rgb, vec3(2.2)), 1.0);
}
</code></pre>
<p>下图是 SRGB 模式的效果，postProcess 选择 c^2.2 能达到同样的效果。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1Aqr8tZbpK1RjSZFyXXX_qFXa-600-419.png" alt="SRGB"></p>
<p>当然，我们也可以勾选 SRGB，然后在 postProcess 中选择取 1/2.2 次方幂，这样输出的颜色和最初的几乎一样。SRGB 扩展将纹理颜色值压低，后处理又把输出的颜色值调亮，两者相互抵消。这正是 PBR 渲染的常用做法，先把纹理颜色从人眼颜色空间的纹理压低成物理空间，然后进行一系列物理规律运算，最后输出时再调亮成人眼颜色空间。</p>
<p>下图是 SRGB + postProcess c^1/2.2 的效果，和不使用 SRGB 和 postProcess 的效果基本一致：</p>
<p><img src="https://gw.alicdn.com/tfs/TB1Qr__tYvpK1RjSZFqXXcXUVXa-600-421.png" alt="SRGB_PostProcess"></p>
<h2 id="tex_lod-">Tex_Lod 扩展</h2>
<p>之前说过，从 Mipmap 中的哪一级纹理取色，取决于在此纹理上采样的密度。但是有了 Tex_Lod 扩展，可以不受此限制，在着色器中直接编码指定从某一级纹理取色。</p>
<p>开启 Tex_Lod 扩展：</p>
<pre><code class="lang-javascript">gl.getExtension(&quot;EXT_shader_texture_lod&quot;);
</code></pre>
<p>着色器中也需要加入开启 #extension 宏：</p>
<pre><code class="lang-glsl">#extension GL_EXT_shader_texture_lod: enable
</code></pre>
<p>在着色器中，可调用 <code>texture2DLodExt</code> 函数来从指定级别的 Mipmap 上取色。</p>
<pre><code class="lang-glsl">gl_FragColor = texture2DLodEXT(uTexture, st, uExtLodLevel);
</code></pre>
<p>在右上角的控制器中，确保 Mipmap 相关的选项都打开，然后打开 extLod，拖动 extLodLevel 滑块，可见虽然纹理本身没有在缩放，但是纹理似乎变模糊了，此时实际上就是取到级别较高的 Mipmap 中去了，因为 Mipmap 的级别越高，像素数量越少，在尺寸不变的情况下，就会变模糊了。</p>
<p>下图是 extLod 开启后，在不依赖纹理的缩放导致采样密度变化的情况下，直接手动编码来从不同级别的 Mipmap 上取色的情况：</p>
<p><img src="https://gw.alicdn.com/tfs/TB1xK7vtVYqK1RjSZLeXXbXppXa-500-316.gif" alt="extLod"></p>
<h2 id="-">小结</h2>
<p>至此，完成了上述所有配置项的讨论。</p>
<p>（完）</p>
<script async src="https://static.jsbin.com/js/embed.min.js?4.1.7"></script>]]></description>
            <link>http://xieguanglei.github.io/post/webgl-texture.html</link>
            <guid isPermaLink="false">webgl-texture</guid>
            <dc:creator><![CDATA[谢光磊]]></dc:creator>
            <pubDate>Tue, 04 Dec 2018 00:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[「译」论孩子]]></title>
            <description><![CDATA[<h1 id="-">「译」论孩子</h1>
<blockquote>
<p>今天在网上闲逛时，无意读到纪伯伦的诗《论孩子》（冰心翻译的版本）。这首诗之前应也曾读过，当时没有什么特别感受；但有了自己的孩子之后，再读此诗，竟有些不一样的感觉。冰心翻译的版本，我觉得有点过于直白，过于追求「直译」了，于是我再对照着英文原文，按着自己的理解意译了一个自己的版本，仅作娱乐。</p>
<p><strong>以下是我的译文：</strong></p>
</blockquote>
<h2 id="-">论孩子</h2>
<p>你的孩子，既是你的孩子，也不是你的孩子。</p>
<p>她是生命的精灵，借你而来，却不因你而来；</p>
<p>她陪伴你于左右，受你养育，却不从属于你。</p>
<p>你可以给予她以关爱，却无法给予她以思想；</p>
<p>你可以庇护她的身体，却不能庇护她的灵魂；</p>
<p>因为她的灵魂，属于明天，那个你在梦中都无法到达的明天。</p>
<p>你可以努力地模仿她，却无法迫使她模仿你，</p>
<p>正如生命时钟不可倒转，昨日时光从未停留。</p>
<p>你是弓，而她是搭在弓弦上的箭矢，</p>
<p>神明已在暗中看清了命运，他用神力拉满弓弦，箭矢便轻快地飞向远方。</p>
<p>愿你与她的缘分，成为生命中一场难忘欢愉，</p>
<p>愿神明眷顾着你，也眷顾着她。</p>
<p>（完）</p>
<blockquote>
<p><strong>附上原文：</strong></p>
<h2 id="on-children">On Children</h2>
<p>Your children are not your children.</p>
<p>They are the sons and daughters of Life’s longing for itself.</p>
<p>They come through you but not from you,</p>
<p>And though they are with you, yet they belong not to you.</p>
<p>You may give them your love but not your thought,</p>
<p>For they have their own thoughts.</p>
<p>You may house their bodies but not their soul,</p>
<p>For their souls dwell in the house of tomorrow,</p>
<p>Which you cannot visit, not even in your dreams.</p>
<p>You may strive to be like them,</p>
<p>But seek not to make them like you,</p>
<p>For life goes not backward nor tarries with yesterday.</p>
<p>You are the bows from which your children,</p>
<p>As living arrows are sent forth.</p>
<p>The archer sees the mark upon the path of the infinite,</p>
<p>And He bends you with His might,</p>
<p>That His arrows may go swift and far.</p>
<p>Let your bending in the archer’s hand be for gladness;</p>
<p>For even as He loves the arrow that flies,</p>
<p>So He loves also the bow that is stable.</p>
<p><strong>附上冰心先生翻译的版本：</strong></p>
<h2 id="-">论孩子</h2>
<p>你们的孩子，都不是你们的孩子，</p>
<p>乃是「生命」为自己所渴望的儿女。</p>
<p>他们是借你们而来，却不是从你们而来，</p>
<p>他们虽和你们同在，却不属于你们。</p>
<p>你们可以给他们以爱，却不可给他们以思想，</p>
<p>因为他们有自己的思想。</p>
<p>你们可以荫庇他们的身体，却不能荫庇他们的灵魂，</p>
<p>因为他们的灵魂，是住在「明日」的宅中，那是你们在梦中也不能想见的。</p>
<p>你们可以努力去模仿他们，却不能使他们来像你们，</p>
<p>因为生命是不倒行的，也不与「昨日」一同停留。</p>
<p>你们是弓，你们的孩子是从弦上发出的生命的箭矢。</p>
<p>那射者在无穷之中看定了目标，也用神力将你们引满，使他的箭矢迅疾而遥远地射了出去。</p>
<p>让你们在射者手中的「弯曲」成为喜乐吧；</p>
<p>因为他爱那飞出的箭，也爱那静止的弓。</p>
</blockquote>
]]></description>
            <link>http://xieguanglei.github.io/post/on-children.html</link>
            <guid isPermaLink="false">on-children</guid>
            <dc:creator><![CDATA[谢光磊]]></dc:creator>
            <pubDate>Fri, 23 Nov 2018 00:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[逐个像素的艺术 —— 2018 iWeb 峰会演讲（全文）]]></title>
            <description><![CDATA[<h1 id="-2018-iweb-">逐个像素的艺术 —— 2018 iWeb 峰会演讲（全文）</h1>
<p><img src="https://gw.alicdn.com/tfs/TB1lboDj3ZC2uNjSZFnXXaxZpXa-800-450.jpg" alt="001"></p>
<p>大家好。很荣幸能够站在这个舞台上。我今天演讲的主题是《逐个像素的艺术》。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1k46zoRnTBKNjSZPfXXbf1XXa-800-450.jpg" alt="002"></p>
<p>先简单作一下自我介绍吧，我来自淘宝网，大家知道阿里巴巴大部分员工都是有花名的，我在公司内的花名是「叶斋」。</p>
<p>我大概从 2013 年开始接触前端图形技术，包括 HTML5 / CSS3 / canvas / webgl 等等，后来我还翻译了一本书，叫《WebGL 编程指南》，如果有学习过原生 WebGL 的，我想应该听说过或者看过这本书吧。</p>
<p>我毕业后，一直在淘宝的前端团队工作，目前负责手机淘宝 App 内部的前端图形渲染基础能力的建设。同时我还维护着一个开源的 WebGL 引擎 G3D。</p>
<p>我的博客，还有邮箱都在这里。如果这次分享之后，大家还有想和我交流的，欢迎给我发邮件。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1ygOco_CWBKNjSZFtXXaC3FXa-800-450.jpg" alt="003"></p>
<p>我读高中的时候，遇到一位音乐老师。他曾经在北朝鲜待过一段时间，所以曾在课上播放过一些北朝鲜的官方大型文艺演出节目的片段。北朝鲜的大型文艺演出有一个特点，就是规模特别大，演出的人很多，但是动作及为整齐，其中最能给我留下深刻印象的，就是「人工大屏」。</p>
<p>什么是「人工大屏」呢？就是有很多很多，估计有上万人，站在类似体育馆看台的台阶上，每个人举一块小牌子，牌子上的颜色各不相同。这样从远处看，这些人就组成了一块显示屏。这块显示屏内容可以跟着演出的节奏进行切换，有时候还可以播放动画，非常整齐。</p>
<p>当时我就想，这演出的背后得，付出多大的代价去训练。因为这种情况，只要有一个人不协调，那就是非常明显的。而要把上万个人训练得没有一个人出错，难度有多高。我觉得，这不是大力出奇迹就能做到的，背后应该有极为严密的组织和极为合理的方法，才能把上万人训练成这样。这是怎么做到的呢？当时我就百思不得其解。直到后来，我学习了 WebGL 和 OpenGL，我才理解到，其实 WebGL 和 OpenGL 所做的事情和训练这么多人是一样的。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1oRGIoGAoBKNjSZSyXXaHAVXa-800-450.jpg" alt="004"></p>
<p>回到这次演讲的题目。我演讲的题目是《逐个像素的艺术》，那什么是像素呢？其实刚刚我说的「人工大屏」，其中每个人其实就是一个像素。单个像素的行为是非常简单的，就是去显示单个颜色。但是，当像素的数量膨胀，达到相当规模的时候，就构成了图像，构成了电影、游戏，呈现了丰富多彩的世界。</p>
<p>而 WebGL / OpenGL 的核心能力，就是「逐个像素地生成颜色」。这里有个误区，就是很多同学一听到 WebGL，第一反应就是，这是用来做 3D，做游戏的。其实呢，WebGL 本身跟 3D 没有太大的关系，它只是提供了「逐像素绘图」的能力，3D 相关的逻辑是更上层的 Shader 层和 JavaScript 层处理的。</p>
<p>我想在场的，大部分是前端程序员，我们可以回想一下，在面向 UI 的编程过程中，我们所操作的最小单元是什么？是一个按钮，一个 input 框，对不对？至于这个按钮，这个 input 框里面的结构是什么样的，这是浏览器本身实现的，我们并没有太多办法去改变。如果我们想要获得像素级别的控制，只能依赖 canvas 标签。</p>
<p>那么熟悉 canvas 的同学可能会说了，canvas 2d 绘图上下文也具有「逐像素绘图」的能力，对不对。我们知道，canvas 2d context 有一个 <code>putImageData</code> 方法。通过这个方法，我们可以去构造一个 UInt16Array，然后向里面填入颜色，每四个值表示一个像素，RGBA。确实，这条路行得通，但是太慢了，一个 100 x 100 的 canvas，这个 canvas 其实尺寸已经很小了。这样一个 canvas 每一帧要循环 100 乘以 100，也就是 10000 次。但是 WebGL 不一样，WebGL 可以利用显卡 GPU 加速的能力，每一个像素的颜色由一个单独的 GPU 核心运算。我们知道 GPU 的核的数量非常多，可以并发进行大量的简单的运算。有个段子，说 GPU 和 CPU 有什么区别，它俩的区别就是 1 个博士生和 1000 个小学生的区别。现在要进行 1000 次简单的四则运算，请问是 1 个博士生算得快，还是 1000 个小学生算得快？那肯定是 1000 个小学生算得快，对吧，因为 1000 个小学生可以进行大量的并发运算。</p>
<p>那么我这次演讲的题目，《逐个像素的艺术》，主要呢就是分享我个人学习、实践 WebGL 的一些心得、体会，以及 WebGL 技术在手机淘宝内部的应用情况。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1ZHF4o5MnBKNjSZFzXXc_qVXa-800-450.jpg" alt="005"></p>
<p>我们看一下 WebGL 在 Google 上的搜索热度趋势。WebGL 差不多 09 年开始有一些草案，11 年标准正式发布，一时间可以说是万众瞩目，可以说那个时候，大家对 WebGL 的期待是很高的，都觉得这技术能够彻底颠覆 Web 的形式。但是呢，随着时间的推移，WebGL 热度，大家可以看到，不仅没升，反而稳中有降。从 11 年到现在这么多年，WebGL 似乎并没有什么里程碑式的产品，更没有像当初大家预期的那样，使 Web 发生翻天覆地的变化。</p>
<p>我们知道前端技术的迭代周期是很快的，逆水行舟，不进则退，如果说 React 的搜索热度趋势和上面这张图差不多的话，那我估计 React 离完蛋不远了。那是不是说，WebGL 就要完蛋了呢？当然不是。否则我也不会站在这里，对吧？</p>
<p><img src="https://gw.alicdn.com/tfs/TB1IGpVo2ImBKNjSZFlXXc43FXa-800-450.jpg" alt="006"></p>
<p>WebGL 并没有完蛋！</p>
<p>首先 WebGL 是一种非常底层的技术，注定了是很慢热的。技术深度比较深，从人才到技术的积累速度都比传统前端技术慢很多，但相对来说技术的过时也会慢很多。</p>
<p>随着无线化的全面到来，PC 由消费产品向生产力工具转变的趋势，我认为这也许会给 WebGL 带来新的机遇。我们知道 WebGL 的功能始终只是 OpenGL ES 的子集，OpenGL ES 又是 OpenGL 的子集，所以 WebGL 在与桌面游戏竞争时，画面效果一直是出于下风的，对游戏来说，画面效果是极为重要的。另一方面，Web 化并未给桌面游戏带来什么太大的好处，为了玩到一个 3A 级大作，比如《巫师3》这种，用户完全有耐心去进行桌面游戏的安装和更新操作，在浏览器里玩对用户来说并没有太大的价值。</p>
<p>但是，生产力工具不一样。随着各类 ERP 系统，特定领域的管理工具（比如家居家装设计，医疗影像等等）从桌面迁移到 Web，WebGL 将会成为这些工具在 Web 上进行图形渲染的唯一选择。而且，生产力工具的图形渲染需求，对画面的要求没有游戏那么高，而迁移到 Web 这件事带来的好处，又会比游戏强很多。</p>
<p>一个明证就是，新一代更加专业的 WebGL 引擎 Babylon.js 的出现，在很多 Web 生产力工具中得到应用。</p>
<p>所以我认为，在今天，WebGL 仍然是值得学习的。</p>
<p>但是我在学习 WebGL 过程中发现，对于没有太多基础的前端工程师而言，入门 WebGL 是一件非常困难的事情。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1C95YoIUrBKNjSZPxXXX00pXa-800-450.jpg" alt="007"></p>
<p>我总结了一下，对一个初学者来说，有三座大山需要去爬。分别是数学基础，渲染管线和状态机，3D 建模知识。下面我简单介绍下：</p>
<p><img src="https://gw.alicdn.com/tfs/TB143X7oY3nBKNjSZFMXXaUSFXa-800-450.jpg" alt="008"></p>
<p>首先是数学基础。不知道在座的各位，高中立体几何有没有全部还给老师。我这里给大家出了一道题。左边我画了一个坐标系，有 X 轴 Y 轴和 Z 轴，然后空间中有一个点 A，从 A 点向几个面和轴作垂线。然后角 AOB 是 45 度，角 BOP 是 30 度，，求 A 点的坐标是多少。</p>
<p>（互动环节，第一个回答上来的同学送一个淘公仔，以下解释答案）。</p>
<p>A 点的坐标是 (0.866, 1, 0.5)，具体是怎么算的呢？AO 是 1.414，也就是 根号2，角 AOB 是 45 度，所以我们知道 AB 和 OB 的场地都是 1，AB 的长度就是坐标 Y 分量的值。然后角 BOP 是 30 度，我们知道 sin(30°) 是 0.5，那么 BP 就是 0.5，OP 是根号3除以2，也就是 0.866。最后的答案是 (0.866, 1, 0.5)。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1IBuWoHwrBKNjSZPcXXXpapXa-800-450.jpg" alt="009"></p>
<p>下面我们来看另一个问题，就是如何描述一个点在空间中的变换，所谓变换就是指平移，旋转和缩放。举个例子，一个点 P 的坐标是 (x, y, z)，平移 (a, b, c)，也就是沿 X 轴平移 a，沿 Y 轴平移 b，沿 Z 轴平移 c，求平移后的点的坐标。</p>
<p>这个问题其实很简单，对吧，答案是 (x+a, y+b, z+c)，方法也很简单，就是简单的矢量加法，各个分量相加就可以。</p>
<p>但是在 WebGL 中，并不是这样计算的，而是像右边这样，使用一个矩阵来计算。首先我们给点的坐标加上一位 1，得到 (x, y, z, 1)，然后使用这样一个矩阵来乘列向量。4 乘 4 的矩阵乘以 4 维列向量的方法，得到一个新的 4 维列向量，每一个值是矩阵的一行乘以列向量得到的单个值。比如，<code>x+a</code> 是这样算出来的：<code>1*a+0*b+0*c+1*1</code>。</p>
<p>我们发现，使用这个矩阵乘下来的结果和之前平移加和的结果是一样的，那这个矩阵就称之为平移矩阵，是不是很神奇？那使用矩阵有什么好处呢？其实啊，除了平移，旋转和缩放也可以统一用矩阵来描述，这样就可以将不同的变换统一为一个格式来描述。而且，当多个变换复合的时候，比如先旋转再平移，我们也可以将旋转矩阵和平移矩阵相乘得到的新矩阵，来描述「先旋转再平移」这么一个复合的变换，非常方便。</p>
<p>那使用矩阵还有一个好处，就是矩阵不仅可以变换位置，还可以变换矢量。上面我们给点 P 的坐标多加了一位 1，如果我们多加的一位是 0，会怎样？加一位 0，表示这个 x,y,z 表示的是一个空间中的一个方向，而不是位置，而平移一个方向，方向本身是不会变的，矩阵乘下来，因为这一位是 0 了，所以也不会变。</p>
<p>熟悉 CSS3 的同学，都知道 CSS3 有个 transform 属性，我们可以使用 translate, rotate 等关键字来描述变换，我想大家应该都用得很溜吧。其实仔细看文档的同学，就会发现还可以使用一个叫 matrix 的关键字来描述变换，这里其实就是使用变换矩阵来描述，这种方式更加直接，实际上浏览器内部也是使用矩阵来描述的。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1nB47oYArBKNjSZFLXXc_dVXa-800-450.jpg" alt="010"></p>
<p>刚刚我举了这俩例子，只是图形学所需要数学知识的冰山一角。当你想要描述旋转的时候，你可能要用到欧拉角，四元数；对模型进行变换的时候，需要考虑是在本地坐标系还是在世界坐标系中进行；相机里也涉及到很多数学知识，包括逆矩阵，不同类型的投影矩阵等等等等。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1uTpyo8smBKNjSZFFXXcT9VXa-800-450.jpg" alt="011"></p>
<p>下面来看第二座大山，渲染管线和 WebGL 状态机。渲染管线这个词不知道大家听说过没有，它是 WebGL 的核心。它提供了一个「友好」的对显卡工作原理的描述，我认为理解了渲染管线，就基本理解了 WebGL 的本质。</p>
<p>这里简单解释一下，举个例子，我要绘制这么一个三角形，那么在 JavaScript 环境中，我们需要构建出一些结构性的数据，来描述这个三角形，其中最重要的就是顶点位置信息，这里三个点 ABC 分别是 1,0,1,3,2,0,0,3,0.5。将这些结构性数据发送给 vertexShader 顶点着色器，顶点着色器是一小段用 glsl 编写的程序，在初始化的时候由 JavaScript 动态编译然后放在渲染管线里面。经过顶点着色器处理，三角形的顶点位置发生了变化，它们会被变换到 CCV 标准立方体中，标准立方体是一个在 X，Y，Z 轴上均在 -1 到 1 的这么一个，边长为 2 的立方体。</p>
<p>然后，数据经过一个名为「光栅化」的过程，三角形就被转化为了屏幕中的一些像素，也就是说这些像素需要被「着色」。</p>
<p>然后再经过片元着色器的处理，片元着色器和顶点着色器一样，也是 JavaScript 初始化的时候放置到渲染管线中的。经过偏远着色器的处理，每一个像素就被「上色」了，最终绘制得到一个红色的三角形。</p>
<p>这里我其实还是讲得比较简单的，还有一些环节（比如深度检测等等）没有涉及。但是呢，如果你能够弄明白渲染管线运行的整个过程，明白 WebGL 的每个 API 究竟是在操作渲染管线的哪个部分，那我觉得啊，你学习 WebGL 已经开始入门了。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1qMd7oY3nBKNjSZFMXXaUSFXa-800-450.jpg" alt="012"></p>
<p>除了渲染管线，你还需要去了解 WebGL 状态机。这么说吧，我们可以把 WebGL 看成是一个大机器，这个机器的产出，就是每一帧生成一张图片，那么这个机器运行需要一些原料，机器上面还有很多开关。通常，机器运行的流程是这样的：</p>
<p>在初始化的时候，我们要准备原料：这些原料包括主要包括着色器程序，数据块，纹理等等。原料的准备是比较耗时的；然后，就开始了每一帧的绘制。我们知道一般来说我们 1 秒钟会渲染 24 帧，如果帧数降低的话，就会给人卡的感觉。</p>
<p>在每一帧，我们做的事情包括，第一步，是去把原料和机器连接起来，这个操作的开销是很低的，大家可以理解为把指针指向原料；第二步，是去操作机器上的各种开关，这些开关可能是离散状态的，就像普通的开关一样，也有可能是需要你输入浮点数，可以把他理解为滑块型的开关；第三部，命令机器，开动起来，也就是去调一次 draw call（drawElements 或者 drawArray）。如此三步，就可以绘制场景中的一部分了。一帧有可能会重复多次上述的流程，最后就这一帧的图像就绘制出来了。</p>
<p>可以看到，每一帧的操作其实性能开销是比较低的，初始化时性能开销很高。那么基本上遵循下面这个原则，就可以让 WebGL 应用保持比较在一个比较好的水平了：首先，不要在每一帧中去处理物料本身；其次，尽可能减少每一帧 drawCall 的次数，当然代价就是初始化构建原料时的复杂度可能会增加。</p>
<p>理解了 WebGL 状态机，再去操作 WebGL API，我相信你就会有种成竹在胸，游刃有余的感觉了。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1QmXhoYorBKNjSZFjXXc_SpXa-800-450.jpg" alt="013"></p>
<p>最后一座大山，是 3D 建模知识。我们不可能始终使用 JavaScript 代码来构建渲染场景需要用到的模型。通常，要借助一些三维建模软件，比如 blender，maya，3d max 甚至 sketchup 等等来。我想如果有志于搞 3D 编程的话，至少学会使用一款 3D 建模软件，进而理解 3D 模型的结构。只有理解 3D 模型的结构，才可能进一步选择合适自己的模型格式。</p>
<p>虽然模型格式有多种多样，比如 obj，stl，fbx，gltf，但是最基本的结构是一致的。举个例子：我有一个模型，就是一个正方形，处于 X-Y 轴这个平面上。它的模型里包含了如下这些信息：一是顶点的坐标，毋庸置疑，这里有四个顶点，所以使用长度为 12 的一个数组来表示；二呢，是法线数据，法线是极为重要的，是光照的基础，这里法线是和顶点一一对应的，都是指向 Z 轴正方向，也就是 (0,0,1)；三是 UV 数据，因为我们这边贴了一张纹理，UV 数据表示顶点与纹理坐标的对应关系，比如 A 这个点对应在纹理图片中是左上角，所以 A 这个点的 UV 是 (0,0)；四是顶点索引数据，因为通常我们是通过绘制三角形来绘制模型，这里的正方形 ABCD 其实是通过绘制两个三角形 ABC 和 ACD 来完成的，索引 [0,1,2,0,2,3] 表示的就是绘制三角形的顺序。当然最后我们还是用到一张图片纹理。以上这些，就是用来表示一个模型最基础的数据结构。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1i0R3oVooBKNjSZFPXXXa2XXa-800-450.jpg" alt="014"></p>
<p>不同的渲染算法，可能会对模型格式提出不同的要求。比如 PBR 渲染（基于物理规则的渲染），就要求模型具有诸如粗糙度，金属度之类的信息。下面是 G3D 渲染的一个经典的 PBR 头盔 demo，这里模型中除了基本的顶点数据，UV 数据，还会使用多张纹理来表示不同的参数，比如基地色；粗糙度/金属度，这里把两个参数合并在了一张纹理的两个通道中；法线，这里法线其实是一个修正量，用来修正跟着顶点的法线，会获得更加细腻的效果；还有发光分量等等。</p>
<p>所以，当我们渲染的物体，材质越来越复杂，算法越来越复杂的时候，实际上模型本身也需要去做出合适的改变。</p>
<p><img src="https://gw.alicdn.com/tfs/TB11JF3oVooBKNjSZFPXXXa2XXa-800-450.jpg" alt="015"></p>
<p>这样，我们就把三座大山全部过完了。除了这三座大山，在入门 WebGL 的过程中，还有一些比较小的门槛，或者说小山坡吧。不过我相信，如果你连前面三座大山都能克服下来，下面这些问题应该不大会阻碍你了。比如说，我们要去熟悉 WebGL 的古怪风格的 API，做任何事情都要先 bind 一下；我们要去学习 GLSL 的语法，这是编写着色器的语言，不过如果你有一些 C 语言基础的话，这应该不是什么难事；比如，我们需要掌握 WebGL 调试的一些方法，尤其是调试 Shader 的一些方法，在 Shader 里面不能 <code>console.log</code>，通常需要一些特殊的技巧把一些中间结果给输出出来；当然，因为 WebGL 项目所需要管理的规模会越来越大，管理的资源的种类可能也会越来越多，所以对前端工程能力也有一定的要求，至少 Webpack 得用得比较溜，各种资源，还有着色器源码的拼接，内联这些工作，都是可以放在编译时来完成的。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1GBqWoHwrBKNjSZPcXXXpapXa-800-450.jpg" alt="016"></p>
<p>当你翻过了这三座大山，也克服了这些小山，你就算精通 WebGL 了吗？我想再给大家泼一盆冷水，其实这时候，也才算是刚刚入门而已。当你掌握了上面这些知识并能熟练运用，你就算是走进了图形渲染技术这座花园的大门，这座花园里有着数不尽的奇珍异宝，你可以自如地把它们拿过来把玩把玩。你可以去更深入地去阅读书籍和文献，去探寻比如水体该如何实现，宝石该如何实现这些一个一个具体又精妙的问题，然后尝试用你手上的工具，WebGL 来实践。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1BM4wo_qWBKNjSZFAXXanSpXa-800-450.jpg" alt="017"></p>
<p>下面讲一讲图形渲染技术在手机淘宝内部的使用，也就是我们团队所做的一些工作。我们团队，是淘宝技术部的终端架构团队，leader 是大家熟悉的 winter 老师。我们团队现在主要是做两个体系，一个是 UI 体系，一个是图形体系。UI 体系主要就是包含 Weex 相关的事情，图形体系主要是 GCanvas 和 G3D。那我们今天分享的主题是 WebGL，所以重点呢是 G3D，但是说到 G3D，不得不提到 GCanvas，而说到 GCanvas 又不得不提到 Weex，那我们就从 Weex 开始说起。</p>
<p>在过去的一年多里，手机淘宝内发生了一个全面 Weex 化的过程。Weex 是淘系应用，包括手机淘宝，手机天猫，里面的一个基础技术框架，Weex 有点类似于 React Native，通过摒弃 WebView 来提高界面渲染的性能和功能。现在大家在手机淘宝里看到的绝大多数页面都已经是 Weex 的了。</p>
<p>但是全面 Weex 化之后，发现了一个问题，就是 Weex 下没有 Canvas 标签。可是使用 Canvas 的需求仍然存在，尤其是遇到营销活动，比如双十一双十二，这时候需求会很多。这时候，我们团队就发展了一个叫做 GCanvas 的产品，目标呢，就是提供一个符合 W3C 标准的 API 的，Weex 环境下的 Canvas。GCanvas 既支持 2d 绘图，也支持 webgl 绘图。</p>
<p>有了 GCanvas 之后呢，我们就像作一些尝试，在 GCanvas 上来渲染一些 3d 场景，一开始我们试着把 babylon.js 等一些已有的 3D 引擎接进来，后来发现行不通，因为这些引擎都依赖了大量的浏览器 API，比如说它会调用 <code>document.createElement</code> 来创建离屏的 canvas 来进行一些预处理，比如它会发起 ajax 请求去加载资源和图片，等等，它的设计和架构就是为浏览器量身定制的。而我们需要的是一个纯粹的，除了 canvas 以外，不依赖其他浏览器 API 的一个渲染引擎。于是我们就写了重新写了一个 3D 引擎，叫 G3D。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1Z8uWoHwrBKNjSZPcXXXpapXa-800-450.jpg" alt="018"></p>
<p>这里我简单地做了一个对比，最左边的是 Web 应用，一般会依赖一个 3D 渲染引擎比如 three.js，babylon.js，由引擎去调用 canvas 的 webgl 绘图上下文，通过浏览器调用系统的图形 API 也就是 OpenGL；而纯 native 的应用，比如手机游戏，一般会用一个大而全的框架，比如 unity，这个框架做的不仅仅是渲染这一层了，还包含很多其他，比如游戏逻辑等等。对于手机淘宝这样的混合型 App，可以依赖一个对标 three.js 的框架，也就是我们的 G3D，然后去调用一个对标 canvas 的 GCanvas，最终还是调用 OpenGL ES。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1DlnJoJcnBKNjSZR0XXcFqFXa-800-450.jpg" alt="019"></p>
<p>看一下 G3D 提供的功能，主要分为四块：底层功能是不以 API 的形式开放给开发者的，包括物料管理，状态机管理，场景树，节点变换等等；基础功能包括相机，元几何体，像立方体，球体，圆柱圆锥等等，不同的光照，不同的材质；交互动画，点选拖拽；最上面是插件功能，主要包含对各种模型的解析，包括解析 OBJ 格式的模型，STL 格式的，字体，还有 GLTF 格式的。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1MftooZj_B1NjSZFHXXaDWpXa-800-450.jpg" alt="020"></p>
<p>这是 G3D 的一些 demo 的演示，分别是透视和正射相机、网格、平行光与点光、元几何体、原始材质 RawMaterial、点选、拖拽、顶点形变动画、PBR 材质、阴影、三种不同的格式模型；最后两个是 PBR 渲染的模型，上面一个是戒指，下面是一个头盔。这个头盔 demo 也是比较经典的用来验证 PBR 渲染的一个案例。</p>
<p><img src="https://gw.alicdn.com/tfs/TB18RqWoHwrBKNjSZPcXXXpapXa-800-450.jpg" alt="021"></p>
<p>下面是 G3D 的系统架构，这个大家看一看就可以了。值得注意的是，G3D 虽然号称完全不依赖浏览器 API，但是实现过程有一些确实是没办法绕过的，比如 Image 和 Video 这样的对象。这里 G3D 是通过依赖注入的方式来完成解耦，就是初始化 G3D 的时候把这些对象注入进 G3D。</p>
<p><img src="https://gw.alicdn.com/tfs/TB1N_lyo8smBKNjSZFFXXcT9VXa-800-450.jpg" alt="022"></p>
<p>这样我的演讲就结束了，谢谢大家！最后，按照惯例要放这个的，就是说：我们招人。</p>
<p>我们是淘宝技术部，终端架构团队，由 winter 老师亲自带领的队伍，负责维护 Weex / Rax / Binding X / GCanvas / G3D 等多个淘系应用中的基础框架，现在跪求：一个是资深前端工程师/专家，一个是资深无线工程师/专家，如果有图形渲染，图像处理等背景，就更好了!</p>
<p>好的，谢谢大家！</p>
<p>（完）</p>
]]></description>
            <link>http://xieguanglei.github.io/post/2018-iweb-speech.html</link>
            <guid isPermaLink="false">2018-iweb-speech</guid>
            <dc:creator><![CDATA[谢光磊]]></dc:creator>
            <pubDate>Sun, 12 Aug 2018 00:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[活用 Shader，让你的页面更小，更炫，更快]]></title>
            <description><![CDATA[<h1 id="-shader-">活用 Shader，让你的页面更小，更炫，更快</h1>
<p>可编程着色器（shader）是运行在 GPU 中的程序，是现代图形渲染技术的基础。Shader 赋予了开发者「逐像素着色」的能力。桌面/移动设备的图形程序 API 诸如 OpenGL，OpenGL ES，DirectX 以及新一代的 Vulkan，shader 都是重中之重，核心中的核心。</p>
<p>WebGL 的出现，使得在浏览器环境中渲染 3D 场景变得轻而易举。但是 WebGL 和 shader 不仅可以用来渲染 3D 场景，还可以做一些其他酷酷的事情。前两天，我用 shader 技术改造 / 复刻了之前开发的一个业务页面，颇有心得和启发，不妨记录下来。</p>
<blockquote>
<p>广告：在 GCanvas 的帮助下，前端开发可以在 Weex，RN 等 Hybrid 环境中使用本文中用到的技术。详情见 <a href="https://alibaba.github.io/GCanvas/">GCanvas</a>。</p>
</blockquote>
<p>先看一下效果：<a href="http://g.alicdn.com/gama/assets/0.0.10/assets/shader-view-demo/compare.html">链接</a></p>
<p><img src="https://img.alicdn.com/tfs/TB1RNl_qntYBeNjy1XdXXXXyVXa-600-529.jpg" alt=""></p>
<p>左侧是原页面，<a href="http://g.alicdn.com/gama/assets/0.0.10/assets/shader-view-demo/index.html">地址</a>；右侧是用 Shader 复刻后的页面，<a href="http://g.alicdn.com/gama/assets/0.0.10/assets/shader-view-demo/shader.html">地址</a>。</p>
<p>这其实是 2018 年春晚项目的一个活动页面，页面结构非常简单。这个页面当时是我完成的，所以现在复刻起来熟悉一些。</p>
<p>我们可以看到，复刻前的页面（后面称「原页面」）是静态的，加载了 1 个 js 文件和 6 张图片共 599K 的资源（包含一张 502K 的大尺寸透明 png 图片）；而复刻后的页面上，有不少元素在动，加载了 1 个 js 文件和 4 张图片共 122K 的资源。不管是视觉效果，还是页面尺寸上的提升，都是比较明显的。</p>
<p><img src="https://img.alicdn.com/tfs/TB1B88_qntYBeNjy1XdXXXXyVXa-500-254.jpg" alt=""></p>
<p><img src="https://img.alicdn.com/tfs/TB1Xcsip1uSBuNjy1XcXXcYjFXa-500-226.jpg" alt=""></p>
<p>下面，我们就以这个页面为例，分析一下，使用 Shader 是如何让这个页面更小，更炫，更快。</p>
<blockquote>
<p>阅读后面的文本需要一些 webgl 和 glsl 的基础知识，之前在团内对曾做过一些培训，参加的同学应该不会有什么压力，没参加的同学，也可以稍微看下 the book of shaders 这篇教程。Shader 比你想象的要简单易用，相信我。</p>
</blockquote>
<h1 id="-">大尺寸透明背景图</h1>
<p>原页面存在的一个最大的问题是，有一张特别大的透明背景图。</p>
<p><img src="https://img.alicdn.com/tfs/TB11dUjp1uSBuNjy1XcXXcYjFXa-750-571.jpg" alt=""></p>
<p>这张图的体积达到了惊人的 501K，这是因为这张图是具有透明通道的 png 图片。而且由于这张图是广告内容，可能不止一张，是无法融合到背景里去的，必须透明。这时怎么优化呢？
我们知道，具有透明通道的 png 的压缩是比较困难的；而不具备透明通道的图片，我们可以把它转化为 jpg 等格式，压缩比就高得多了，我们就可以以较小的质量损失去换取较大的压缩空间。</p>
<p>我的思路是这样：把这张透明的 png 格式图片拆分为两张不透明的 jpg 格式图片。这两张不透明的图片，其中一张继承 png 图片的 rgb 通道，还有一张则仅使用 r 通道储存 png 图片的 a 通道。然后把这两张图片拼接在一起，给 WebGL 使用。由于拼接后的这张图没有透明度分量，所以可以使用 jpg 格式压缩，尺寸大幅度降低。这张图只有 41.5k，大约为之前的 8.2%。</p>
<p>这张图看上去是这样的：</p>
<p><img src="https://img.alicdn.com/tfs/TB13tUjp1uSBuNjy1XcXXcYjFXa-700-700.jpg" alt=""></p>
<blockquote>
<p>注意，前一张图的像素尺寸是 750x571，而后一张图的像素尺寸为 1024x1024，这里我并没有通过缩小图片的像素尺寸来进行压缩。</p>
<p>此外，第二张图看上去有些变形，这是因为图片尺寸为 2 的整数次幂，WebGL 能够方便地生成 mipmap，这对我们的使用没有影响。</p>
</blockquote>
<p>在 shader 中，我们根据像素坐标从图片中取色，注意需要从图的上半部分和下半部分各取一个颜色，然后根据一定规则拼起来即可。</p>
<pre><code class="lang-glsl">precision mediump float;

uniform vec2 uResolution;
uniform sampler2D uImage;

void main(){

    vec2 st = gl_FragCoord.xy / uResolution;

    vec4 c2 = texture2D(uImage, vec2(st.x, st.y*0.5));      // 取 A​lpha 通道
    vec4 c1 = texture2D(uImage, vec2(st.x, st.y*0.5+0.5));  // 取 RGB 通道

    gl_FragColor = vec4(c1.xyz, c2.r &gt; 0.6 ? c2.r : 0.0);
}
</code></pre>
<p>png 图片转化为 jpg 图片的过程，可以很轻松地在浏览器里操作 canvas 完成（示例），也可以借助一些其他的工具完成。</p>
<h1 id="-">会动的背景</h1>
<p>首先，我们注意到，原页面的背景是在红色的渐变之上，随机散布着一些黄色的氛围小碎片。红色渐变背景和这些小碎片全部画在一张静态 jpg 图片上，如下图（1.原图）所示。</p>
<p><img src="https://img.alicdn.com/tfs/TB16JUjp1uSBuNjy1XcXXcYjFXa-700-1256.jpg" alt=""></p>
<p>在复刻前，我把原页面用到的图片分为了两类，图案（pattern）性质和图片（image）性质。Pattern 性质的图片，本身并不传递信息，通常用作底纹，氛围等场景；而 image 性质的图片则是信息的载体。</p>
<p>这张图片明显是 pattern 性质的，这类图片往往尺寸大，体积也较大（尤其是半透明图案）。其实，这些图案完全可以用 Shader 「手绘」出来，这样就不用去加载此图片了。
用 Shader 绘制图案的另一个好处是，图案可以有规律地动起来。在这个例子中，如果碎片能像天女散花一般洒下来，那就太棒了，对吧？但是熟悉前端动画的同学，一定会想到，这么多粒子组成的动画，如果用纯 CSS 或者 canvas 2d 来做的话，性能肯定好不了，粒子越多，动画的性能越差。用 WebGL 和 shader 来做粒子动画则不会因为粒子数量的增多而导致性能变差。</p>
<p>我们来看看如何用 Shader 绘制这些碎片。</p>
<pre><code class="lang-glsl">vec2 random2(vec2 st){
  st = vec2( dot(st,vec2(127.1,311.7)),
            dot(st,vec2(269.5,183.3)));
  st = -1.0 + 2.0*fract(sin(st)*43758.5453123);
  return st;
}


float noise2(vec2 ist, vec2 fst){
  vec2 g1 = random2(ist+vec2(0.0, 0.0));
  vec2 g2 = random2(ist+vec2(1.0, 0.0));
  vec2 g3 = random2(ist+vec2(0.0, 1.0));
  vec2 g4 = random2(ist+vec2(1.0, 1.0));

  vec2 f1 = fst - vec2(0.0, 0.0);
  vec2 f2 = fst - vec2(1.0, 0.0);
  vec2 f3 = fst - vec2(0.0, 1.0);
  vec2 f4 = fst - vec2(1.0, 1.0);

  float p1 = dot(g1, f1);
  float p2 = dot(g2, f2);
  float p3 = dot(g3, f3);
  float p4 = dot(g4, f4);

  fst = smoothstep(0.0, 1.0, fst);

  float p = mix(
    mix(p1, p2, fst.x),
    mix(p3, p4, fst.x),
    fst.y
  );

  return p;
}


float inFrag(){
    vec2 st = gl_FragCoord.xy / uResolution.xx;
    st = st * 60.0;
    float res = noise2(floor(st), fract(st));
    return res;
}

void main(){
    float pct = inFrag();
    gl_FragColor = vec4(vec3(pct), 1.0);
}
</code></pre>
<p>首先我们要借助一个梯度噪声函数 noise2（参考此教程），对每个像素而言，把像素坐标输入，这个函数则会输出一个灰度值。此函数输出的图像大致如上图 （2.梯度噪声）所示。
如果你对诸如「噪声函数」的原理感到陌生，其实也没太大关系。你可以在社区找到大量各种各样的开箱即用的功能函数，只需要知道它们的效果是什么，而不必太拘泥于其内部的原理。
显然，图 2.梯度噪声 和我们设想的还有差距。接下来，我们用一个筛子把亮度大于某个阈值的点筛出来：</p>
<pre><code class="lang-glsl">function initFrag(){
    ...
    res = step(0.5, res);
    return res;
}
</code></pre>
<p>这样，用 step 函数直接把大于 0.5 的点筛出来。可是这样做容易产生锯齿，为了使碎片的边缘比较平滑，所以我们用 smoothsStep 函数进行截取。</p>
<pre><code class="lang-glsl">res = smoothstep(0.35, 0.5, res);
</code></pre>
<p>这样，我们就得到了图 3.拉伸的结果。</p>
<p>图 3 只是一张灰度图，我们使用这个灰度混合红色和黄色，使之得到一张彩色的图。</p>
<pre><code class="lang-glsl">vec3 bgColor(){
    float y = gl_FragCoord.y / uResolution.y;
    vec3 c1 = vec3(0.96, 0.02, 0.16);
    vec3 c2 = vec3(0.96, 0.25, 0.21);
    return mix(c1, c2, y);
}

void main(){
    ...

    vec3 cRed = bgColor();
    vec3 cYello = vec3(0.96, 0.70, 0.26);

    gl_FragColor = vec4(mix(cRed, cYello, pct), 1.0);
}
</code></pre>
<p>这里 bgColor 方法返回红色，由于红色背景仍然是有一点垂直渐变色效果的，所以这里也要额外用两种不同的红色进行混合（混合系数和像素坐标的 Y 值相关），处理成渐变色。</p>
<p>此时我们的结果和原图的意图还有些不同：</p>
<ul>
<li>原图中，页面下半部分的碎片比较透明度，越往页面下方，碎片就越透明（融入了红色背景）。</li>
<li>原图中，中间圈圈部分（即红色窗格占据的部分）没有碎片。</li>
<li>原图中，碎片的分布没有这么均匀，常有一小块区域完全没有碎片的情况，似乎有一种尺寸更大的随机变量在影响。</li>
</ul>
<p>从以上三点出发，我们制作了 3 个通道，并依次叠加（如图 5，图 6，图 7）所示，最终得到如 图 7 所示。将叠加后的结果与图 3 进行叠加，也就是说，图 3 中被筛出的点，如果在图 7 中是较暗的，则也会被降低亮度。再使用这一步的结果进行混色，最终得到图 8 的效果。</p>
<pre><code class="lang-glsl">void main() {

    float pct = inFrag();
    pct = min(pct, yFactor());
    pct = min(pct, rFactor());
    pct = min(pct, mFactor());

    ...
}
</code></pre>
<p>下面，我们来使碎片动起来（洒下来）。在生成碎片的时候，传入噪音函数的坐标数据中，加上和时间有关的偏移量：</p>
<pre><code class="lang-glsl">float inFrag(){
    vec2 st = gl_FragCoord.xy / uResolution.xx;
    st = st * 60.0;
    st.y += uTime * 2.0;  // 增加与时间相关的偏移量
    float res = noise2(floor(st), fract(st));
    res = smoothstep(0.35, 0.5, res);
    return res;
}
</code></pre>
<p>最后，为了更出色的效果，我这里做了两个碎层碎片，两层碎片具有不同的下落速度，形成一些视差效果。</p>
<pre><code class="lang-glsl">void main() {

    float pct = inFrag();
    pct = min(pct, yFactor());
    pct = min(pct, rFactor());
    pct = min(pct, mFactor());

    float pct2 = inFrag2();
    pct2 = min(pct2, yFactor());
    pct2 = min(pct2, rFactor());
    pct2 = min(pct2, mFactor());

    pct = max(pct, pct2);

    ...
}
</code></pre>
<p>这样，就在完全不依赖外部资源的情况下，仅用 Shader 直接绘制，制作出了氛围碎片的效果。</p>
<h1 id="-">手绘图案</h1>
<p>原页面中有一个圆形的窗格，这个窗格也是画在一张透明图片上。不知读者是否注意到，在复刻后的页面中，这个窗格是用 shader 直接画出来的。</p>
<p><img src="https://img.alicdn.com/tfs/TB10Tp.qntYBeNjy1XdXXXXyVXa-400-363.jpg" alt=""></p>
<p>实际上，这种复杂程度的窗格，也可以归为图案（pattern）一类，shader 是完全可以直接画出来的。下面，我们就来看看用 shader 如何来画窗格。
窗格是由线组成的，其基本单元是线。首先我们看一下是如何画线的：</p>
<pre><code class="lang-glsl">// 绘制线的函数 veins
float line(float e, float w, float d, float p){
    float e1 = e - w/2.0;
    float e2 = e + w/2.0;
    return smoothstep(e1 - d / 2.0, e1 + d / 2.0, p) * 
            smoothstep(e2 + d / 2.0, e2 - d / 2.0, p);
}

// 绘制网格
vec3 veins(){
    float r = uResolution.x * 0.4;
    vec2 center = vec2(uResolution.x/2.0, uResolution.y-r-5.0);
    vec2 st = gl_FragCoord.xy - center;
    st /= uResolution.x * 0.5;

    float p = line(0.0, 0.3, 0.2, st.x);

    return mix(veinsBgColor, veinsFgColor, p);
}

// 主函数
void main(){
    vec3 res = veins();

    gl_FragColor = vec4(vec3(res), 1.0);
}
</code></pre>
<p>main 函数调用 veins 函数，veins 又调用 line 函数得到一个灰度值，然后混合两种颜色。上述程序的结果如下图所示。</p>
<p><img src="https://img.alicdn.com/tfs/TB1ctq.p3mTBuNjy1XbXXaMrVXa-400-430.jpg" alt=""></p>
<p>解释一下几个参数：p 是当前像素的 x 或 y 坐标值（取决于横线还是竖线，如果是横线为 y 坐标值，如果为竖线为 x 坐标值），e 则是所绘制的直线所在的坐标。w 指线的宽度，而 d 指在线与非线的交界处，用来平滑的区域的宽度。</p>
<blockquote>
<p>在上面的代码中，w 取了 0.3，而 d 取了 0.2，线看上去很粗。后面，我们会把这两个值固定在 0.035 和 0.003 上。</p>
</blockquote>
<p>由于窗格图案中包含多跟线，我们需要多次调用 line 函数，并得到一个一个灰度值。如果当前像素在「任意一个」 line 函数中返回了大于 0 的灰度值，我们就认为这个像素是在图案上的。换言之，我们取多次 line 函数返回的灰度值中最大的那个值，作为最后的灰度值来计算颜色。代码如下所示：</p>
<pre><code class="lang-glsl">float maxList(float list[20]){
    float res = list[0];
    for(int i=0; i&lt;20; i++){
        if(list[i]&gt;res){
            res = list[i];
        }
    }
    return res;
}

vec3 veins(){
    ...

    float p = 0.0;
    float pl[20];
    pl[0] = line(0.29, 0.035, 0.003, st.x);
    pl[1] = line(0.58, 0.035, 0.003, st.x);
    ...
    pl[7] = line(-0.58, 0.035, 0.003, st.y);

    p = maxList(pl);

    ...
}
</code></pre>
<p>我们计算了 8 根直线，得到的结果如下图 2 所示。</p>
<p><img src="https://img.alicdn.com/tfs/TB1ltq.p3mTBuNjy1XbXXaMrVXa-700-1105.jpg" alt=""></p>
<p>拆解图案，我们发现光有直线还不能满足要求，还需要有射线和矩形框。同样，我们引入射线 ray 和矩形框 box 函数。</p>
<pre><code class="lang-glsl">float rayV(vec2 ep, float w, float d,  float dir, vec2 st){
    float pct = line(ep.x, w, d, st.x);
    if((st.y - ep.y) * dir &lt; 0.0){
        pct = 0.0;
    }
    return pct;
}

float rayH(vec2 ep, float w, float d,  float dir, vec2 st){
    float pct = line(ep.y, w, d, st.y);
    if((st.x - ep.x)* dir &lt; 0.0){
        pct = 0.0;
    }
    return pct;
}

float box(vec2 center, float width, float height, float w, float d, vec2 st){

    float l1 = line(center.x, width+w, d, st.x);
    float l2 = line(center.y, height+w, d, st.y);

    float inBox = l1*l2;
    float plist[20];

    plist[0] = line(center.x+width*0.5, w, d, st.x);
    plist[1] = line(center.x-width*0.5, w, d, st.x);
    plist[2] = line(center.y+height*0.5, w, d, st.y);
    plist[3] = line(center.y-height*0.5, w, d, st.y);

    float p = maxList(plist);
    p *= inBox;
    return p;
}
</code></pre>
<p>然后依次向图案中增加内容，得到图 4，图 6 的效果。通过最终的叠加，得到了图 7 的效果。代码如下（不要被密密麻麻的浮点数吓住了，其实都是一些固定的坐标而已，有意义的值只有几个，通过正负号进行组合形成图案）：</p>
<pre><code class="lang-glsl">    float p = 0.0;
    float pl[20];
    pl[0] = line(0.29, 0.035, 0.003, st.x);
    pl[1] = line(0.58, 0.035, 0.003, st.x);
    pl[2] = line(-0.29, 0.035, 0.003, st.x);
    pl[3] = line(-0.58, 0.035, 0.003, st.x);
    pl[4] = line(0.29, 0.035, 0.003, st.y);
    pl[5] = line(0.58, 0.035, 0.003, st.y);
    pl[6] = line(-0.29, 0.035, 0.003, st.y);
    pl[7] = line(-0.58, 0.035, 0.003, st.y);

    pl[8] = rayV(vec2(0.0, 0.29), 0.035, 0.003, 1.0, st);
    pl[9] = rayV(vec2(0.0, -0.29), 0.035, 0.003, -1.0, st);
    pl[10] = rayH(vec2(0.29, 0.0), 0.035, 0.003, 1.0, st);
    pl[11] = rayH(vec2(-0.29, 0.0), 0.035, 0.003, -1.0, st);

    p = maxList(pl);

    float pl2[20];

    pl2[0] = box(vec2(0.0, 0.0), 0.39, 0.39, 0.035, 0.003, st);

    pl2[1] = box(vec2(0.29, 0.29), 0.39, 0.39, 0.035, 0.003, st);
    pl2[2] = box(vec2(-0.29, 0.29), 0.39, 0.39, 0.035, 0.003, st);
    pl2[3] = box(vec2(-0.29, -0.29), 0.39, 0.39, 0.035, 0.003, st);
    pl2[4] = box(vec2(0.29, -0.29), 0.39, 0.39, 0.035, 0.003, st);

    pl2[5] = box(vec2(0.58, 0.0), 0.39, 0.39, 0.035, 0.003, st);
    pl2[6] = box(vec2(-0.58, 0.0), 0.39, 0.39, 0.035, 0.003, st);
    pl2[7] = box(vec2(0.0, 0.58), 0.39, 0.39, 0.035, 0.003, st);
    pl2[8] = box(vec2(0.0, -0.58), 0.39, 0.39, 0.035, 0.003, st);

    pl2[9] = box(vec2(0.58, 0.58), 0.39, 0.39, 0.035, 0.003, st);
    pl2[10] = box(vec2(-0.58, 0.58), 0.39, 0.39, 0.035, 0.003, st);
    pl2[11] = box(vec2(-0.58, -0.58), 0.39, 0.39, 0.035, 0.003, st);
    pl2[12] = box(vec2(0.58, -0.58), 0.39, 0.39, 0.035, 0.003, st);

    p = max(p, maxList(pl2));
</code></pre>
<p>得到图 7 的图案后，我们还需要为其蒙上一层阴影（可对比原图），这样后面裁切的时候会有一些立体感。</p>
<pre><code class="lang-glsl">float shadow(){
    float r = uResolution.x * 0.4;
    vec2 center = vec2(uResolution.x/2.0, uResolution.y-r-5.0);
    vec2 st = gl_FragCoord.xy - center;
    st /= uResolution.x * 0.5;

    return smoothstep(0.9, 0.3, st.y+0.5*st.x*st.x-0.1);
}

vec3 veins(){
    return mix(veinsBgColor, veinsFgColor, p)*shadow();
}
</code></pre>
<p>这里为了方便，使用了一个开口朝下，中轴和 y 轴重合的抛物线（st.y + 0.5<em>st.x</em>st.x - 0.1）来模拟圆形的阴影。这样我们就得到了图 8。</p>
<p>最后，原设计稿中红色边框和透明背景的效果，对整个图像进行了两次裁切。裁切掉的部分，分别用红色和透明色来填充。依次得到图 9 和 图 10 的结果。图 10 也就是最终的结果。</p>
<pre><code class="lang-glsl">vec3 circle(vec3 veinsColor){

    float r = uResolution.x * 0.4;    
    vec2 center = vec2(uResolution.x/2.0, uResolution.y-r-5.0);

    vec2 dxy = gl_FragCoord.xy - center;
    float dist = sqrt(dxy.x*dxy.x+dxy.y*dxy.y);

    float p = dist/r;
    p = smoothstep(0.95, 0.96, p);

    return mix(veinsColor, borderColor, p);
}

vec4 clip(vec3 color){
    float r = uResolution.x * 0.4;    
    vec2 center = vec2(uResolution.x/2.0, uResolution.y-r-5.0);

    vec2 dxy = gl_FragCoord.xy - center;
    float dist = sqrt(dxy.x*dxy.x+dxy.y*dxy.y);

    float p = smoothstep(1.0, 1.02, dist/r);

    return vec4(color, 1.0-p);
}


void main(){
    vec3 res = veins();
    res = circle(res);

    gl_FragColor = clip(res);
}
</code></pre>
<h1 id="-">结语</h1>
<p>通过上面三个例子，可以看到，合理地使用 webgl 可以对页面进行精雕细琢的优化，可以减少对图片的依赖，避免使用大尺寸的透明图层，可以做一些全局性/背景性的动画效果。由于 webgl 是给了开发者「逐个像素」进行着色的能力，开发者可以非常灵活地使用 shader 来做事情。所以说，灵活地使用 shader ，可以帮助你把页面变得更小，更炫，更快。</p>
<p>其实复刻后的页面里还有一些其他用 shader 完成的小玩意儿，比如底部 loading bar 的动态颜色渐变，以及中部文字「魅族手机祝你新春快乐」上掠过的高光，因为点比较小，用到的技术也比较简单，就不再详细介绍了。</p>
<p>（完）</p>
]]></description>
            <link>http://xieguanglei.github.io/post/using-shader-to-optimize-page.html</link>
            <guid isPermaLink="false">using-shader-to-optimize-page</guid>
            <dc:creator><![CDATA[谢光磊]]></dc:creator>
            <pubDate>Thu, 10 May 2018 00:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[G3D —— Hybrid 环境下的 WebGL 3D 渲染引擎]]></title>
            <description><![CDATA[<h1 id="g3d-hybrid-webgl-3d-">G3D —— Hybrid 环境下的 WebGL 3D 渲染引擎</h1>
<blockquote>
<p>G3D 是一款开源 3D 渲染引擎，目前由我开发和维护。这篇宣传最初发表于<a href="http://taobaofed.org/blog/2018/03/05/intro-to-g3d/">淘宝前端团队博客</a>。这里是个人博客上保存的一份备份。</p>
</blockquote>
<p>G3D 是一款基于 WebGL 的 JavaScript 3D 渲染引擎，借助 GCanvas，G3D 可以运行在 Weex，ReactNative 等 hybrid 环境下。G3D 由淘宝终端团队推出，并于 2018 年 3 月与 GCanvas 同时宣布正式开源。</p>
<p>那么就会有同学问了，G3D 和 three.js 有什么不同呀？G3D 和 GCanvas 究竟是什么关系啊？这篇文章，就聊一聊 G3D 这个产品的来龙去脉。</p>
<p><a href="https://alibaba.github.io/G3D/">G3D 官网</a>，<a href="https://alibaba.github.io/GCanvas/">GCanvas 官网</a></p>
<h2 id="-g3d">为什么有 G3D</h2>
<p>G3D 的起源要从 GCanvas 说起。</p>
<p>GCanvas 在 Weex 和 ReactNative 环境下提供了浏览器环境中 Canvas 的绘图能力，手机淘宝 App 的 Weex 容器已经内置了 GCanvas。和 Canvas 一样，GCanvas 的绘图能力包括 2d 上下文和 webgl 上下文的绘图能力。2d 上下文相对较为简单，可以直接拿来使用；而 webgl 上下文比较复杂，从 webgl API 到真正的 3d 应用之间往往还需要一层 3d 渲染引擎，社区中的 three.js，babylon.js 等就是这类 3d 渲染引擎中的翘楚。</p>
<p>GCanvas 开发团队曾尝试把 three.js 和 babylon.js 接入到 GCanvas 环境中来，遇到了一些困难：</p>
<ul>
<li>社区中的 webgl 渲染引擎依赖了大量的 DOM API 和原生对象，在 Weex 与 ReactNative 环境中不存在这些 API 与原生对象。开发团队也曾尝试对 Babylon.js 和 three.js 进行改造，但发现成本比较高，而且后续跟进原版项目 bugfix 与功能迭代的难度也比较大。</li>
<li>如 GCanvas 文档所述， GCanvas 目前仅支持 WebGL API 的一个子集。直接引入 Babylon.js 和 three.js，在 GCanvas环境下还暂时无法正常工作。</li>
<li>由于 three.js 和 babylon.js 的体积已经比较巨大，其中很多功能在手机淘宝的业务场景中暂时用不到。因此，即使能够成功改造，巨大的 js 体积也会拖垮手淘中很多页面的性能。</li>
</ul>
<p>所以，GCanvas 开发团队决定从零开始开发一个小型的 WebGL 渲染引擎 G3D，并以此作为 GCanvas 3D 能力的辅助。可以预见，G3D 和 GCanvas WebGL 将会是相辅相成，互相促进，共同发展；并且在较长一段时间内，G3D 将是使用 GCanvas WebGL 能力，除了直接操作原生 WebGL API 之外的唯一选择。</p>
<h2 id="g3d-">G3D 有哪些功能</h2>
<p>G3D 具有 3D 渲染引擎的基本功能：</p>
<ul>
<li>定义场景，定义透视相机。</li>
<li>光照方面，目前支持 1)环境光；2)平行光；3)点光；4)穹顶光。</li>
<li>材质方面，目前支持 1)基于冯氏反射模型的冯式面材质（朗伯面是冯氏面的一种特殊情况）；2) 非光照材质。</li>
<li>几何体方面，目前支持直接创建的几何体包括立方体，球体，圆柱，圆锥，折线；当然更多情况下是可以通过解析模型数据创建几何体。</li>
<li>模型解析方面，目前支持 1) OBJ/MTL 模型；2) STL 模型这两种模型格式。</li>
<li>交互：支持 3D点选/拖拽（由于 GCanvas framebuffer 仍未正常，此功能仅在浏览器中有效）。</li>
<li>动画：支持骨骼动画和蒙皮动画。</li>
</ul>
<p>值得注意的是，由于 G3D 需要运行在 Hybrid 环境下，无法依赖 DOM API，所以与 three.js，babylon.js 等浏览器环境的引擎相比，G3D 无法支持诸如声音播放，文件加载等非渲染核心功能。举例来说，如果使用 three.js 加载模型，只需要调用相关方法传入模型的 url 即可，three.js 会自动加载和解析模型；但在使用 G3D 时，你需要手动获取该文件的内容（Hybrid 与浏览器会不一样），然后将内容字符串传入 G3D.MeshBuilder 的相关方法。</p>
<h2 id="g3d-">G3D 的未来</h2>
<p>目前 G3D 已经在淘宝 3D 定制等业务中有所使用，在未来的半年到一年里，G3D 主要的目的有两个：</p>
<ul>
<li>追赶 Babylon.js 和 three.js 的高阶功能，如阴影、Shader材质、预处理、法线纹理、光线追踪等等，在其过程中推动 GCanvas WebGL 的完善，同时赋能业务和社区。</li>
<li>进一步打通 G3D 与建模-动画工具链的通道，完善模型-材质数据结构，提升渲染效果，达到「（Blender 等工具中）所见即（G3D渲染出）所得」的开发体验，最大程度地降低 3D 项目的开发成本。</li>
</ul>
]]></description>
            <link>http://xieguanglei.github.io/post/about-g3d.html</link>
            <guid isPermaLink="false">about-g3d</guid>
            <dc:creator><![CDATA[谢光磊]]></dc:creator>
            <pubDate>Wed, 07 Mar 2018 00:00:00 GMT</pubDate>
        </item>
    </channel>
</rss>